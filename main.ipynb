{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e19b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75bb523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda:0\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print(\"Device: \", device)\n",
    "\n",
    "N = 100\n",
    "batch_size = 128\n",
    "z_dim = 30  # noise dimension\n",
    "num_epochs = 300\n",
    "critic_num = 2 # Update critic _ before generator updates\n",
    "lambd = 10 # Penalization constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad94ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.load(\"X_data.npy\")\n",
    "Y_data = np.load(\"Y_data.npy\")\n",
    "\n",
    "X_tensor = torch.tensor(X_data, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y_data, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Split train and test set\n",
    "Y_train, Y_test, X_train, X_test = train_test_split(Y_tensor, X_tensor, test_size=0.2)\n",
    "\n",
    "train_dataset = TensorDataset(Y_train, X_train)\n",
    "test_dataset = TensorDataset(Y_test, X_test)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f10d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sin(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.sin(x)\n",
    "\n",
    "# Define Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, y_dim, z_dim, x_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(y_dim + z_dim, 128),\n",
    "            Sin(),\n",
    "            nn.Linear(128, 128),\n",
    "            Sin(),\n",
    "            nn.Linear(128, x_dim)\n",
    "        )\n",
    "    def forward(self, y, z):\n",
    "        input = torch.cat([y, z], dim=1)\n",
    "        return self.net(input)\n",
    "\n",
    "# Define Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, y_dim, x_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(y_dim + x_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, y, x):\n",
    "        input = torch.cat([y, x], dim=1)\n",
    "        return self.net(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d76a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(y_dim=N, z_dim=z_dim, x_dim=N).to(device)\n",
    "D = Discriminator(y_dim=N, x_dim=N).to(device)\n",
    "\n",
    "# Print total params\n",
    "print(sum(p.numel() for p in G.parameters() if p.requires_grad))\n",
    "print(sum(p.numel() for p in D.parameters() if p.requires_grad))\n",
    "\n",
    "# Optimizer\n",
    "lr = 5e-3\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=lr)\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=lr)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler_G = StepLR(optimizer_G, step_size=50, gamma=0.8)\n",
    "scheduler_D = StepLR(optimizer_D, step_size=50, gamma=0.8)\n",
    "\n",
    "# Loss\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626917e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    G.train()\n",
    "    D.train()\n",
    "    for Y_batch, X_batch in train_loader:\n",
    "        Y_batch = Y_batch.to(device)\n",
    "        X_batch = X_batch.to(device)\n",
    "        batch_size_cur = Y_batch.shape[0]\n",
    "\n",
    "        # Train Discriminator\n",
    "        #--------------------\n",
    "        optimizer_D.zero_grad()\n",
    "        real_labels = torch.ones(batch_size_cur, 1, device=device)\n",
    "        fake_labels = torch.zeros(batch_size_cur, 1, device=device)\n",
    "\n",
    "        d_real = D(Y_batch, X_batch)\n",
    "        loss_d_real = criterion(d_real, real_labels)\n",
    "\n",
    "        z = torch.randn(batch_size_cur, z_dim, device=device)\n",
    "        X_fake = G(Y_batch, z)\n",
    "        d_fake = D(Y_batch, X_fake.detach())\n",
    "        loss_d_fake = criterion(d_fake, fake_labels)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss_d = loss_d_real + loss_d_fake\n",
    "        loss_d.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        #----------------\n",
    "        if epoch % critic_num == 0:\n",
    "          optimizer_G.zero_grad()\n",
    "          d_fake = D(Y_batch, X_fake)\n",
    "          # Calculate loss\n",
    "          loss_g = criterion(d_fake, real_labels)\n",
    "          loss_g += lambd * F.mse_loss(X_fake, X_batch) # penalize generator\n",
    "          loss_g.backward()\n",
    "          optimizer_G.step()\n",
    "    # LR scheduler update\n",
    "    scheduler_G.step()\n",
    "    scheduler_D.step()\n",
    "\n",
    "    if epoch %50 == 0 :\n",
    "        print(f\"Epoch {epoch}: D loss {loss_d.item():.4f}, G loss {loss_g.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27d8213",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.eval()\n",
    "num_samples = 100\n",
    "num_plots = 6\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axs = axs.flatten()\n",
    "with torch.no_grad():\n",
    "    for i, (Y_sample, X_sample) in enumerate(test_loader):\n",
    "        if i >= num_plots:\n",
    "            break\n",
    "        Y_sample = Y_sample.to(device)\n",
    "        X_sample = X_sample.to(device)\n",
    "        generated_samples = []\n",
    "        for _ in range(num_samples): # generate multiple x given y\n",
    "            z = torch.randn(1, z_dim, device=device)\n",
    "            X_gen = G(Y_sample, z).cpu().numpy().flatten()\n",
    "            generated_samples.append(X_gen)\n",
    "\n",
    "        generated_samples = np.array(generated_samples)\n",
    "\n",
    "        mean_gen = generated_samples.mean(axis=0) # get mean of generated x\n",
    "        std_gen = generated_samples.std(axis=0)# get SD of generated x\n",
    "\n",
    "        t = np.linspace(0, 1, N)\n",
    "\n",
    "        # Plot true vs generated\n",
    "        ax = axs[i]\n",
    "        ax.plot(t, X_sample.cpu().numpy().flatten(), label='True')\n",
    "        ax.plot(t, mean_gen, label='Mean Generated', linestyle='--')\n",
    "        ax.fill_between(s, mean_gen - 2*std_gen, mean_gen + 2*std_gen, color='grey',  label='95% CI')\n",
    "        plt.suptitle(\"True vs Generated x\", fontsize=18)\n",
    "        ax.set_title(f\"Sample {i+1}\")\n",
    "        ax.set_xlabel(\"t\")\n",
    "        ax.set_ylabel(\" x\")\n",
    "        ax.legend()\n",
    "plt.tight_layout(pad=1.0)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
